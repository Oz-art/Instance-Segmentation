{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vehicle_parts_detection.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oz-art/Instance-Segmentation/blob/main/Vehicle_parts_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f3QR9bNxipPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc854003-1d76-4497-8c16-8d1a08e171aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies: (use cu111 because colab has CUDA 11.1)\n",
        "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# install mmcv-full thus we could use CUDA operators\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
        "\n",
        "# Install mmdetection\n",
        "!rm -rf mmdetection\n",
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "%cd mmdetection\n",
        "\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSuzjNq4jQOb",
        "outputId": "39de61b8-361e-4ceb-88c4-8040ae131f19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[K     |█████████████                   | 834.1 MB 1.5 MB/s eta 0:13:18tcmalloc: large alloc 1147494400 bytes == 0x561b837d6000 @  0x7f902d63b615 0x561b4ac273bc 0x561b4ad0818a 0x561b4ac2a1cd 0x561b4ad1cb3d 0x561b4ac9e458 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9e2c0 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ad1d986 0x561b4ac9a350 0x561b4ad1d986 0x561b4ac9a350 0x561b4ad1d986 0x561b4ac9a350 0x561b4ac2bf19 0x561b4ac6fa79 0x561b4ac2ab32 0x561b4ac9e1dd 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac99eae 0x561b4ac2b9da 0x561b4ac9a108 0x561b4ac9902f\n",
            "\u001b[K     |████████████████▌               | 1055.7 MB 1.5 MB/s eta 0:10:59tcmalloc: large alloc 1434370048 bytes == 0x561bc7e2c000 @  0x7f902d63b615 0x561b4ac273bc 0x561b4ad0818a 0x561b4ac2a1cd 0x561b4ad1cb3d 0x561b4ac9e458 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9e2c0 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ad1d986 0x561b4ac9a350 0x561b4ad1d986 0x561b4ac9a350 0x561b4ad1d986 0x561b4ac9a350 0x561b4ac2bf19 0x561b4ac6fa79 0x561b4ac2ab32 0x561b4ac9e1dd 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac99eae 0x561b4ac2b9da 0x561b4ac9a108 0x561b4ac9902f\n",
            "\u001b[K     |█████████████████████           | 1336.2 MB 1.4 MB/s eta 0:08:17tcmalloc: large alloc 1792966656 bytes == 0x561b4cc5e000 @  0x7f902d63b615 0x561b4ac273bc 0x561b4ad0818a 0x561b4ac2a1cd 0x561b4ad1cb3d 0x561b4ac9e458 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9e2c0 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ad1d986 0x561b4ac9a350 0x561b4ad1d986 0x561b4ac9a350 0x561b4ad1d986 0x561b4ac9a350 0x561b4ac2bf19 0x561b4ac6fa79 0x561b4ac2ab32 0x561b4ac9e1dd 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac99eae 0x561b4ac2b9da 0x561b4ac9a108 0x561b4ac9902f\n",
            "\u001b[K     |██████████████████████████▌     | 1691.1 MB 1.3 MB/s eta 0:04:23tcmalloc: large alloc 2241208320 bytes == 0x561bb7a46000 @  0x7f902d63b615 0x561b4ac273bc 0x561b4ad0818a 0x561b4ac2a1cd 0x561b4ad1cb3d 0x561b4ac9e458 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9e2c0 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ad1d986 0x561b4ac9a350 0x561b4ad1d986 0x561b4ac9a350 0x561b4ad1d986 0x561b4ac9a350 0x561b4ac2bf19 0x561b4ac6fa79 0x561b4ac2ab32 0x561b4ac9e1dd 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac99eae 0x561b4ac2b9da 0x561b4ac9a108 0x561b4ac9902f\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 2041348096 bytes == 0x561c3d3a8000 @  0x7f902d63a1e7 0x561b4ac5d5d7 0x561b4ac273bc 0x561b4ad0818a 0x561b4ac2a1cd 0x561b4ad1cb3d 0x561b4ac9e458 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9a108 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9a108 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9a108 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9a108 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9a108 0x561b4ac2b9da 0x561b4ac9a108 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ac9902f\n",
            "tcmalloc: large alloc 2551685120 bytes == 0x561d2b2f4000 @  0x7f902d63b615 0x561b4ac273bc 0x561b4ad0818a 0x561b4ac2a1cd 0x561b4ad1cb3d 0x561b4ac9e458 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9a108 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9a108 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9a108 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9a108 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9a108 0x561b4ac2b9da 0x561b4ac9a108 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ac9902f 0x561b4ac2baba 0x561b4ac9acd4 0x561b4ac9902f 0x561b4ac2c151\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 7.4 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.10.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (1.21.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.0+cu111 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0+cu111 torchvision-0.10.0+cu111\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
            "Collecting mmcv-full\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/mmcv_full-1.4.6-cp37-cp37m-manylinux1_x86_64.whl (45.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 45.2 MB 9.1 MB/s \n",
            "\u001b[?25hCollecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (1.21.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (3.13)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (4.1.2.30)\n",
            "Collecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv-full) (3.0.7)\n",
            "Installing collected packages: yapf, addict, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.4.6 yapf-0.32.0\n",
            "Cloning into 'mmdetection'...\n",
            "remote: Enumerating objects: 23588, done.\u001b[K\n",
            "remote: Total 23588 (delta 0), reused 0 (delta 0), pack-reused 23588\u001b[K\n",
            "Receiving objects: 100% (23588/23588), 35.32 MiB | 26.06 MiB/s, done.\n",
            "Resolving deltas: 100% (16485/16485), done.\n",
            "/content/mmdetection\n",
            "Obtaining file:///content/mmdetection\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mmdet==2.22.0) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmdet==2.22.0) (1.21.5)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from mmdet==2.22.0) (2.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mmdet==2.22.0) (1.15.0)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.22.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.22.0) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.22.0) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.22.0) (2.8.2)\n",
            "Installing collected packages: terminaltables, mmdet\n",
            "  Running setup.py develop for mmdet\n",
            "Successfully installed mmdet-2.22.0 terminaltables-3.1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mmcv import collect_env\n",
        "collect_env()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5ivMOasjXaz",
        "outputId": "1427d4c7-8877-4967-a069-839e045461e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CUDA available': True,\n",
              " 'CUDA_HOME': '/usr/local/cuda',\n",
              " 'GCC': 'gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0',\n",
              " 'GPU 0': 'Tesla K80',\n",
              " 'MMCV': '1.4.6',\n",
              " 'MMCV CUDA Compiler': '11.1',\n",
              " 'MMCV Compiler': 'GCC 7.3',\n",
              " 'NVCC': 'Build cuda_11.1.TC455_06.29190527_0',\n",
              " 'OpenCV': '4.1.2',\n",
              " 'PyTorch': '1.9.0+cu111',\n",
              " 'PyTorch compiling details': 'PyTorch built with:\\n  - GCC 7.3\\n  - C++ Version: 201402\\n  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\\n  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)\\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\\n  - NNPACK is enabled\\n  - CPU capability usage: AVX2\\n  - CUDA Runtime 11.1\\n  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\\n  - CuDNN 8.0.5\\n  - Magma 2.5.2\\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \\n',\n",
              " 'Python': '3.7.12 (default, Jan 15 2022, 18:48:18) [GCC 7.5.0]',\n",
              " 'TorchVision': '0.10.0+cu111',\n",
              " 'sys.platform': 'linux'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "%cd '/content/mmdetection'\n",
        "mmdetection_dir = '/content/mmdetection'\n",
        "coco_file = os.path.join(mmdetection_dir, \"mmdet/datasets/coco.py\")\n",
        "\n",
        "classes_names = ['_background_','back_bumper', 'back_glass', 'back_left_door', 'back_left_light', 'back_right_door', 'back_right_light', 'front_bumper', 'front_glass', 'front_left_door', 'front_left_light', 'front_right_door', 'front_right_light', 'hood', 'left_mirror', 'right_mirror', 'tailgate', 'trunk', 'wheel']\n",
        "\n",
        "fname = coco_file\n",
        "with open(fname) as f:\n",
        "    s = f.read()\n",
        "    s = re.sub('CLASSES = \\(.*?\\)',\n",
        "               'CLASSES = ({})'.format(\", \".join([\"\\'{}\\'\".format(name) for name in classes_names])), s, flags=re.S)\n",
        "with open(fname, 'w') as f:\n",
        "    f.write(s)\n",
        "!cat {coco_file}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-lTpxy2E-EH",
        "outputId": "f1e9489a-f221-4d2e-94fd-0ccf73a5affb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mmdetection\n",
            "# Copyright (c) OpenMMLab. All rights reserved.\n",
            "import contextlib\n",
            "import io\n",
            "import itertools\n",
            "import logging\n",
            "import os.path as osp\n",
            "import tempfile\n",
            "import warnings\n",
            "from collections import OrderedDict\n",
            "\n",
            "import mmcv\n",
            "import numpy as np\n",
            "from mmcv.utils import print_log\n",
            "from terminaltables import AsciiTable\n",
            "\n",
            "from mmdet.core import eval_recalls\n",
            "from .api_wrappers import COCO, COCOeval\n",
            "from .builder import DATASETS\n",
            "from .custom import CustomDataset\n",
            "\n",
            "\n",
            "@DATASETS.register_module()\n",
            "class CocoDataset(CustomDataset):\n",
            "\n",
            "    CLASSES = ('_background_', 'back_bumper', 'back_glass', 'back_left_door', 'back_left_light', 'back_right_door', 'back_right_light', 'front_bumper', 'front_glass', 'front_left_door', 'front_left_light', 'front_right_door', 'front_right_light', 'hood', 'left_mirror', 'right_mirror', 'tailgate', 'trunk', 'wheel')\n",
            "\n",
            "    PALETTE = [(220, 20, 60), (119, 11, 32), (0, 0, 142), (0, 0, 230),\n",
            "               (106, 0, 228), (0, 60, 100), (0, 80, 100), (0, 0, 70),\n",
            "               (0, 0, 192), (250, 170, 30), (100, 170, 30), (220, 220, 0),\n",
            "               (175, 116, 175), (250, 0, 30), (165, 42, 42), (255, 77, 255),\n",
            "               (0, 226, 252), (182, 182, 255), (0, 82, 0), (120, 166, 157),\n",
            "               (110, 76, 0), (174, 57, 255), (199, 100, 0), (72, 0, 118),\n",
            "               (255, 179, 240), (0, 125, 92), (209, 0, 151), (188, 208, 182),\n",
            "               (0, 220, 176), (255, 99, 164), (92, 0, 73), (133, 129, 255),\n",
            "               (78, 180, 255), (0, 228, 0), (174, 255, 243), (45, 89, 255),\n",
            "               (134, 134, 103), (145, 148, 174), (255, 208, 186),\n",
            "               (197, 226, 255), (171, 134, 1), (109, 63, 54), (207, 138, 255),\n",
            "               (151, 0, 95), (9, 80, 61), (84, 105, 51), (74, 65, 105),\n",
            "               (166, 196, 102), (208, 195, 210), (255, 109, 65), (0, 143, 149),\n",
            "               (179, 0, 194), (209, 99, 106), (5, 121, 0), (227, 255, 205),\n",
            "               (147, 186, 208), (153, 69, 1), (3, 95, 161), (163, 255, 0),\n",
            "               (119, 0, 170), (0, 182, 199), (0, 165, 120), (183, 130, 88),\n",
            "               (95, 32, 0), (130, 114, 135), (110, 129, 133), (166, 74, 118),\n",
            "               (219, 142, 185), (79, 210, 114), (178, 90, 62), (65, 70, 15),\n",
            "               (127, 167, 115), (59, 105, 106), (142, 108, 45), (196, 172, 0),\n",
            "               (95, 54, 80), (128, 76, 255), (201, 57, 1), (246, 0, 122),\n",
            "               (191, 162, 208)]\n",
            "\n",
            "    def load_annotations(self, ann_file):\n",
            "        \"\"\"Load annotation from COCO style annotation file.\n",
            "\n",
            "        Args:\n",
            "            ann_file (str): Path of annotation file.\n",
            "\n",
            "        Returns:\n",
            "            list[dict]: Annotation info from COCO api.\n",
            "        \"\"\"\n",
            "\n",
            "        self.coco = COCO(ann_file)\n",
            "        # The order of returned `cat_ids` will not\n",
            "        # change with the order of the CLASSES\n",
            "        self.cat_ids = self.coco.get_cat_ids(cat_names=self.CLASSES)\n",
            "\n",
            "        self.cat2label = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}\n",
            "        self.img_ids = self.coco.get_img_ids()\n",
            "        data_infos = []\n",
            "        total_ann_ids = []\n",
            "        for i in self.img_ids:\n",
            "            info = self.coco.load_imgs([i])[0]\n",
            "            info['filename'] = info['file_name']\n",
            "            data_infos.append(info)\n",
            "            ann_ids = self.coco.get_ann_ids(img_ids=[i])\n",
            "            total_ann_ids.extend(ann_ids)\n",
            "        assert len(set(total_ann_ids)) == len(\n",
            "            total_ann_ids), f\"Annotation ids in '{ann_file}' are not unique!\"\n",
            "        return data_infos\n",
            "\n",
            "    def get_ann_info(self, idx):\n",
            "        \"\"\"Get COCO annotation by index.\n",
            "\n",
            "        Args:\n",
            "            idx (int): Index of data.\n",
            "\n",
            "        Returns:\n",
            "            dict: Annotation info of specified index.\n",
            "        \"\"\"\n",
            "\n",
            "        img_id = self.data_infos[idx]['id']\n",
            "        ann_ids = self.coco.get_ann_ids(img_ids=[img_id])\n",
            "        ann_info = self.coco.load_anns(ann_ids)\n",
            "        return self._parse_ann_info(self.data_infos[idx], ann_info)\n",
            "\n",
            "    def get_cat_ids(self, idx):\n",
            "        \"\"\"Get COCO category ids by index.\n",
            "\n",
            "        Args:\n",
            "            idx (int): Index of data.\n",
            "\n",
            "        Returns:\n",
            "            list[int]: All categories in the image of specified index.\n",
            "        \"\"\"\n",
            "\n",
            "        img_id = self.data_infos[idx]['id']\n",
            "        ann_ids = self.coco.get_ann_ids(img_ids=[img_id])\n",
            "        ann_info = self.coco.load_anns(ann_ids)\n",
            "        return [ann['category_id'] for ann in ann_info]\n",
            "\n",
            "    def _filter_imgs(self, min_size=32):\n",
            "        \"\"\"Filter images too small or without ground truths.\"\"\"\n",
            "        valid_inds = []\n",
            "        # obtain images that contain annotation\n",
            "        ids_with_ann = set(_['image_id'] for _ in self.coco.anns.values())\n",
            "        # obtain images that contain annotations of the required categories\n",
            "        ids_in_cat = set()\n",
            "        for i, class_id in enumerate(self.cat_ids):\n",
            "            ids_in_cat |= set(self.coco.cat_img_map[class_id])\n",
            "        # merge the image id sets of the two conditions and use the merged set\n",
            "        # to filter out images if self.filter_empty_gt=True\n",
            "        ids_in_cat &= ids_with_ann\n",
            "\n",
            "        valid_img_ids = []\n",
            "        for i, img_info in enumerate(self.data_infos):\n",
            "            img_id = self.img_ids[i]\n",
            "            if self.filter_empty_gt and img_id not in ids_in_cat:\n",
            "                continue\n",
            "            if min(img_info['width'], img_info['height']) >= min_size:\n",
            "                valid_inds.append(i)\n",
            "                valid_img_ids.append(img_id)\n",
            "        self.img_ids = valid_img_ids\n",
            "        return valid_inds\n",
            "\n",
            "    def _parse_ann_info(self, img_info, ann_info):\n",
            "        \"\"\"Parse bbox and mask annotation.\n",
            "\n",
            "        Args:\n",
            "            ann_info (list[dict]): Annotation info of an image.\n",
            "            with_mask (bool): Whether to parse mask annotations.\n",
            "\n",
            "        Returns:\n",
            "            dict: A dict containing the following keys: bboxes, bboxes_ignore,\\\n",
            "                labels, masks, seg_map. \"masks\" are raw annotations and not \\\n",
            "                decoded into binary masks.\n",
            "        \"\"\"\n",
            "        gt_bboxes = []\n",
            "        gt_labels = []\n",
            "        gt_bboxes_ignore = []\n",
            "        gt_masks_ann = []\n",
            "        for i, ann in enumerate(ann_info):\n",
            "            if ann.get('ignore', False):\n",
            "                continue\n",
            "            x1, y1, w, h = ann['bbox']\n",
            "            inter_w = max(0, min(x1 + w, img_info['width']) - max(x1, 0))\n",
            "            inter_h = max(0, min(y1 + h, img_info['height']) - max(y1, 0))\n",
            "            if inter_w * inter_h == 0:\n",
            "                continue\n",
            "            if ann['area'] <= 0 or w < 1 or h < 1:\n",
            "                continue\n",
            "            if ann['category_id'] not in self.cat_ids:\n",
            "                continue\n",
            "            bbox = [x1, y1, x1 + w, y1 + h]\n",
            "            if ann.get('iscrowd', False):\n",
            "                gt_bboxes_ignore.append(bbox)\n",
            "            else:\n",
            "                gt_bboxes.append(bbox)\n",
            "                gt_labels.append(self.cat2label[ann['category_id']])\n",
            "                gt_masks_ann.append(ann.get('segmentation', None))\n",
            "\n",
            "        if gt_bboxes:\n",
            "            gt_bboxes = np.array(gt_bboxes, dtype=np.float32)\n",
            "            gt_labels = np.array(gt_labels, dtype=np.int64)\n",
            "        else:\n",
            "            gt_bboxes = np.zeros((0, 4), dtype=np.float32)\n",
            "            gt_labels = np.array([], dtype=np.int64)\n",
            "\n",
            "        if gt_bboxes_ignore:\n",
            "            gt_bboxes_ignore = np.array(gt_bboxes_ignore, dtype=np.float32)\n",
            "        else:\n",
            "            gt_bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\n",
            "\n",
            "        seg_map = img_info['filename'].replace('jpg', 'png')\n",
            "\n",
            "        ann = dict(\n",
            "            bboxes=gt_bboxes,\n",
            "            labels=gt_labels,\n",
            "            bboxes_ignore=gt_bboxes_ignore,\n",
            "            masks=gt_masks_ann,\n",
            "            seg_map=seg_map)\n",
            "\n",
            "        return ann\n",
            "\n",
            "    def xyxy2xywh(self, bbox):\n",
            "        \"\"\"Convert ``xyxy`` style bounding boxes to ``xywh`` style for COCO\n",
            "        evaluation.\n",
            "\n",
            "        Args:\n",
            "            bbox (numpy.ndarray): The bounding boxes, shape (4, ), in\n",
            "                ``xyxy`` order.\n",
            "\n",
            "        Returns:\n",
            "            list[float]: The converted bounding boxes, in ``xywh`` order.\n",
            "        \"\"\"\n",
            "\n",
            "        _bbox = bbox.tolist()\n",
            "        return [\n",
            "            _bbox[0],\n",
            "            _bbox[1],\n",
            "            _bbox[2] - _bbox[0],\n",
            "            _bbox[3] - _bbox[1],\n",
            "        ]\n",
            "\n",
            "    def _proposal2json(self, results):\n",
            "        \"\"\"Convert proposal results to COCO json style.\"\"\"\n",
            "        json_results = []\n",
            "        for idx in range(len(self)):\n",
            "            img_id = self.img_ids[idx]\n",
            "            bboxes = results[idx]\n",
            "            for i in range(bboxes.shape[0]):\n",
            "                data = dict()\n",
            "                data['image_id'] = img_id\n",
            "                data['bbox'] = self.xyxy2xywh(bboxes[i])\n",
            "                data['score'] = float(bboxes[i][4])\n",
            "                data['category_id'] = 1\n",
            "                json_results.append(data)\n",
            "        return json_results\n",
            "\n",
            "    def _det2json(self, results):\n",
            "        \"\"\"Convert detection results to COCO json style.\"\"\"\n",
            "        json_results = []\n",
            "        for idx in range(len(self)):\n",
            "            img_id = self.img_ids[idx]\n",
            "            result = results[idx]\n",
            "            for label in range(len(result)):\n",
            "                bboxes = result[label]\n",
            "                for i in range(bboxes.shape[0]):\n",
            "                    data = dict()\n",
            "                    data['image_id'] = img_id\n",
            "                    data['bbox'] = self.xyxy2xywh(bboxes[i])\n",
            "                    data['score'] = float(bboxes[i][4])\n",
            "                    data['category_id'] = self.cat_ids[label]\n",
            "                    json_results.append(data)\n",
            "        return json_results\n",
            "\n",
            "    def _segm2json(self, results):\n",
            "        \"\"\"Convert instance segmentation results to COCO json style.\"\"\"\n",
            "        bbox_json_results = []\n",
            "        segm_json_results = []\n",
            "        for idx in range(len(self)):\n",
            "            img_id = self.img_ids[idx]\n",
            "            det, seg = results[idx]\n",
            "            for label in range(len(det)):\n",
            "                # bbox results\n",
            "                bboxes = det[label]\n",
            "                for i in range(bboxes.shape[0]):\n",
            "                    data = dict()\n",
            "                    data['image_id'] = img_id\n",
            "                    data['bbox'] = self.xyxy2xywh(bboxes[i])\n",
            "                    data['score'] = float(bboxes[i][4])\n",
            "                    data['category_id'] = self.cat_ids[label]\n",
            "                    bbox_json_results.append(data)\n",
            "\n",
            "                # segm results\n",
            "                # some detectors use different scores for bbox and mask\n",
            "                if isinstance(seg, tuple):\n",
            "                    segms = seg[0][label]\n",
            "                    mask_score = seg[1][label]\n",
            "                else:\n",
            "                    segms = seg[label]\n",
            "                    mask_score = [bbox[4] for bbox in bboxes]\n",
            "                for i in range(bboxes.shape[0]):\n",
            "                    data = dict()\n",
            "                    data['image_id'] = img_id\n",
            "                    data['bbox'] = self.xyxy2xywh(bboxes[i])\n",
            "                    data['score'] = float(mask_score[i])\n",
            "                    data['category_id'] = self.cat_ids[label]\n",
            "                    if isinstance(segms[i]['counts'], bytes):\n",
            "                        segms[i]['counts'] = segms[i]['counts'].decode()\n",
            "                    data['segmentation'] = segms[i]\n",
            "                    segm_json_results.append(data)\n",
            "        return bbox_json_results, segm_json_results\n",
            "\n",
            "    def results2json(self, results, outfile_prefix):\n",
            "        \"\"\"Dump the detection results to a COCO style json file.\n",
            "\n",
            "        There are 3 types of results: proposals, bbox predictions, mask\n",
            "        predictions, and they have different data types. This method will\n",
            "        automatically recognize the type, and dump them to json files.\n",
            "\n",
            "        Args:\n",
            "            results (list[list | tuple | ndarray]): Testing results of the\n",
            "                dataset.\n",
            "            outfile_prefix (str): The filename prefix of the json files. If the\n",
            "                prefix is \"somepath/xxx\", the json files will be named\n",
            "                \"somepath/xxx.bbox.json\", \"somepath/xxx.segm.json\",\n",
            "                \"somepath/xxx.proposal.json\".\n",
            "\n",
            "        Returns:\n",
            "            dict[str: str]: Possible keys are \"bbox\", \"segm\", \"proposal\", and \\\n",
            "                values are corresponding filenames.\n",
            "        \"\"\"\n",
            "        result_files = dict()\n",
            "        if isinstance(results[0], list):\n",
            "            json_results = self._det2json(results)\n",
            "            result_files['bbox'] = f'{outfile_prefix}.bbox.json'\n",
            "            result_files['proposal'] = f'{outfile_prefix}.bbox.json'\n",
            "            mmcv.dump(json_results, result_files['bbox'])\n",
            "        elif isinstance(results[0], tuple):\n",
            "            json_results = self._segm2json(results)\n",
            "            result_files['bbox'] = f'{outfile_prefix}.bbox.json'\n",
            "            result_files['proposal'] = f'{outfile_prefix}.bbox.json'\n",
            "            result_files['segm'] = f'{outfile_prefix}.segm.json'\n",
            "            mmcv.dump(json_results[0], result_files['bbox'])\n",
            "            mmcv.dump(json_results[1], result_files['segm'])\n",
            "        elif isinstance(results[0], np.ndarray):\n",
            "            json_results = self._proposal2json(results)\n",
            "            result_files['proposal'] = f'{outfile_prefix}.proposal.json'\n",
            "            mmcv.dump(json_results, result_files['proposal'])\n",
            "        else:\n",
            "            raise TypeError('invalid type of results')\n",
            "        return result_files\n",
            "\n",
            "    def fast_eval_recall(self, results, proposal_nums, iou_thrs, logger=None):\n",
            "        gt_bboxes = []\n",
            "        for i in range(len(self.img_ids)):\n",
            "            ann_ids = self.coco.get_ann_ids(img_ids=self.img_ids[i])\n",
            "            ann_info = self.coco.load_anns(ann_ids)\n",
            "            if len(ann_info) == 0:\n",
            "                gt_bboxes.append(np.zeros((0, 4)))\n",
            "                continue\n",
            "            bboxes = []\n",
            "            for ann in ann_info:\n",
            "                if ann.get('ignore', False) or ann['iscrowd']:\n",
            "                    continue\n",
            "                x1, y1, w, h = ann['bbox']\n",
            "                bboxes.append([x1, y1, x1 + w, y1 + h])\n",
            "            bboxes = np.array(bboxes, dtype=np.float32)\n",
            "            if bboxes.shape[0] == 0:\n",
            "                bboxes = np.zeros((0, 4))\n",
            "            gt_bboxes.append(bboxes)\n",
            "\n",
            "        recalls = eval_recalls(\n",
            "            gt_bboxes, results, proposal_nums, iou_thrs, logger=logger)\n",
            "        ar = recalls.mean(axis=1)\n",
            "        return ar\n",
            "\n",
            "    def format_results(self, results, jsonfile_prefix=None, **kwargs):\n",
            "        \"\"\"Format the results to json (standard format for COCO evaluation).\n",
            "\n",
            "        Args:\n",
            "            results (list[tuple | numpy.ndarray]): Testing results of the\n",
            "                dataset.\n",
            "            jsonfile_prefix (str | None): The prefix of json files. It includes\n",
            "                the file path and the prefix of filename, e.g., \"a/b/prefix\".\n",
            "                If not specified, a temp file will be created. Default: None.\n",
            "\n",
            "        Returns:\n",
            "            tuple: (result_files, tmp_dir), result_files is a dict containing \\\n",
            "                the json filepaths, tmp_dir is the temporal directory created \\\n",
            "                for saving json files when jsonfile_prefix is not specified.\n",
            "        \"\"\"\n",
            "        assert isinstance(results, list), 'results must be a list'\n",
            "        assert len(results) == len(self), (\n",
            "            'The length of results is not equal to the dataset len: {} != {}'.\n",
            "            format(len(results), len(self)))\n",
            "\n",
            "        if jsonfile_prefix is None:\n",
            "            tmp_dir = tempfile.TemporaryDirectory()\n",
            "            jsonfile_prefix = osp.join(tmp_dir.name, 'results')\n",
            "        else:\n",
            "            tmp_dir = None\n",
            "        result_files = self.results2json(results, jsonfile_prefix)\n",
            "        return result_files, tmp_dir\n",
            "\n",
            "    def evaluate(self,\n",
            "                 results,\n",
            "                 metric='bbox',\n",
            "                 logger=None,\n",
            "                 jsonfile_prefix=None,\n",
            "                 classwise=False,\n",
            "                 proposal_nums=(100, 300, 1000),\n",
            "                 iou_thrs=None,\n",
            "                 metric_items=None):\n",
            "        \"\"\"Evaluation in COCO protocol.\n",
            "\n",
            "        Args:\n",
            "            results (list[list | tuple]): Testing results of the dataset.\n",
            "            metric (str | list[str]): Metrics to be evaluated. Options are\n",
            "                'bbox', 'segm', 'proposal', 'proposal_fast'.\n",
            "            logger (logging.Logger | str | None): Logger used for printing\n",
            "                related information during evaluation. Default: None.\n",
            "            jsonfile_prefix (str | None): The prefix of json files. It includes\n",
            "                the file path and the prefix of filename, e.g., \"a/b/prefix\".\n",
            "                If not specified, a temp file will be created. Default: None.\n",
            "            classwise (bool): Whether to evaluating the AP for each class.\n",
            "            proposal_nums (Sequence[int]): Proposal number used for evaluating\n",
            "                recalls, such as recall@100, recall@1000.\n",
            "                Default: (100, 300, 1000).\n",
            "            iou_thrs (Sequence[float], optional): IoU threshold used for\n",
            "                evaluating recalls/mAPs. If set to a list, the average of all\n",
            "                IoUs will also be computed. If not specified, [0.50, 0.55,\n",
            "                0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.\n",
            "                Default: None.\n",
            "            metric_items (list[str] | str, optional): Metric items that will\n",
            "                be returned. If not specified, ``['AR@100', 'AR@300',\n",
            "                'AR@1000', 'AR_s@1000', 'AR_m@1000', 'AR_l@1000' ]`` will be\n",
            "                used when ``metric=='proposal'``, ``['mAP', 'mAP_50', 'mAP_75',\n",
            "                'mAP_s', 'mAP_m', 'mAP_l']`` will be used when\n",
            "                ``metric=='bbox' or metric=='segm'``.\n",
            "\n",
            "        Returns:\n",
            "            dict[str, float]: COCO style evaluation metric.\n",
            "        \"\"\"\n",
            "\n",
            "        metrics = metric if isinstance(metric, list) else [metric]\n",
            "        allowed_metrics = ['bbox', 'segm', 'proposal', 'proposal_fast']\n",
            "        for metric in metrics:\n",
            "            if metric not in allowed_metrics:\n",
            "                raise KeyError(f'metric {metric} is not supported')\n",
            "        if iou_thrs is None:\n",
            "            iou_thrs = np.linspace(\n",
            "                .5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)\n",
            "        if metric_items is not None:\n",
            "            if not isinstance(metric_items, list):\n",
            "                metric_items = [metric_items]\n",
            "\n",
            "        result_files, tmp_dir = self.format_results(results, jsonfile_prefix)\n",
            "\n",
            "        eval_results = OrderedDict()\n",
            "        cocoGt = self.coco\n",
            "        for metric in metrics:\n",
            "            msg = f'Evaluating {metric}...'\n",
            "            if logger is None:\n",
            "                msg = '\\n' + msg\n",
            "            print_log(msg, logger=logger)\n",
            "\n",
            "            if metric == 'proposal_fast':\n",
            "                ar = self.fast_eval_recall(\n",
            "                    results, proposal_nums, iou_thrs, logger='silent')\n",
            "                log_msg = []\n",
            "                for i, num in enumerate(proposal_nums):\n",
            "                    eval_results[f'AR@{num}'] = ar[i]\n",
            "                    log_msg.append(f'\\nAR@{num}\\t{ar[i]:.4f}')\n",
            "                log_msg = ''.join(log_msg)\n",
            "                print_log(log_msg, logger=logger)\n",
            "                continue\n",
            "\n",
            "            iou_type = 'bbox' if metric == 'proposal' else metric\n",
            "            if metric not in result_files:\n",
            "                raise KeyError(f'{metric} is not in results')\n",
            "            try:\n",
            "                predictions = mmcv.load(result_files[metric])\n",
            "                if iou_type == 'segm':\n",
            "                    # Refer to https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/coco.py#L331  # noqa\n",
            "                    # When evaluating mask AP, if the results contain bbox,\n",
            "                    # cocoapi will use the box area instead of the mask area\n",
            "                    # for calculating the instance area. Though the overall AP\n",
            "                    # is not affected, this leads to different\n",
            "                    # small/medium/large mask AP results.\n",
            "                    for x in predictions:\n",
            "                        x.pop('bbox')\n",
            "                    warnings.simplefilter('once')\n",
            "                    warnings.warn(\n",
            "                        'The key \"bbox\" is deleted for more accurate mask AP '\n",
            "                        'of small/medium/large instances since v2.12.0. This '\n",
            "                        'does not change the overall mAP calculation.',\n",
            "                        UserWarning)\n",
            "                cocoDt = cocoGt.loadRes(predictions)\n",
            "            except IndexError:\n",
            "                print_log(\n",
            "                    'The testing results of the whole dataset is empty.',\n",
            "                    logger=logger,\n",
            "                    level=logging.ERROR)\n",
            "                break\n",
            "\n",
            "            cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n",
            "            cocoEval.params.catIds = self.cat_ids\n",
            "            cocoEval.params.imgIds = self.img_ids\n",
            "            cocoEval.params.maxDets = list(proposal_nums)\n",
            "            cocoEval.params.iouThrs = iou_thrs\n",
            "            # mapping of cocoEval.stats\n",
            "            coco_metric_names = {\n",
            "                'mAP': 0,\n",
            "                'mAP_50': 1,\n",
            "                'mAP_75': 2,\n",
            "                'mAP_s': 3,\n",
            "                'mAP_m': 4,\n",
            "                'mAP_l': 5,\n",
            "                'AR@100': 6,\n",
            "                'AR@300': 7,\n",
            "                'AR@1000': 8,\n",
            "                'AR_s@1000': 9,\n",
            "                'AR_m@1000': 10,\n",
            "                'AR_l@1000': 11\n",
            "            }\n",
            "            if metric_items is not None:\n",
            "                for metric_item in metric_items:\n",
            "                    if metric_item not in coco_metric_names:\n",
            "                        raise KeyError(\n",
            "                            f'metric item {metric_item} is not supported')\n",
            "\n",
            "            if metric == 'proposal':\n",
            "                cocoEval.params.useCats = 0\n",
            "                cocoEval.evaluate()\n",
            "                cocoEval.accumulate()\n",
            "\n",
            "                # Save coco summarize print information to logger\n",
            "                redirect_string = io.StringIO()\n",
            "                with contextlib.redirect_stdout(redirect_string):\n",
            "                    cocoEval.summarize()\n",
            "                print_log('\\n' + redirect_string.getvalue(), logger=logger)\n",
            "\n",
            "                if metric_items is None:\n",
            "                    metric_items = [\n",
            "                        'AR@100', 'AR@300', 'AR@1000', 'AR_s@1000',\n",
            "                        'AR_m@1000', 'AR_l@1000'\n",
            "                    ]\n",
            "\n",
            "                for item in metric_items:\n",
            "                    val = float(\n",
            "                        f'{cocoEval.stats[coco_metric_names[item]]:.3f}')\n",
            "                    eval_results[item] = val\n",
            "            else:\n",
            "                cocoEval.evaluate()\n",
            "                cocoEval.accumulate()\n",
            "\n",
            "                # Save coco summarize print information to logger\n",
            "                redirect_string = io.StringIO()\n",
            "                with contextlib.redirect_stdout(redirect_string):\n",
            "                    cocoEval.summarize()\n",
            "                print_log('\\n' + redirect_string.getvalue(), logger=logger)\n",
            "\n",
            "                if classwise:  # Compute per-category AP\n",
            "                    # Compute per-category AP\n",
            "                    # from https://github.com/facebookresearch/detectron2/\n",
            "                    precisions = cocoEval.eval['precision']\n",
            "                    # precision: (iou, recall, cls, area range, max dets)\n",
            "                    assert len(self.cat_ids) == precisions.shape[2]\n",
            "\n",
            "                    results_per_category = []\n",
            "                    for idx, catId in enumerate(self.cat_ids):\n",
            "                        # area range index 0: all area ranges\n",
            "                        # max dets index -1: typically 100 per image\n",
            "                        nm = self.coco.loadCats(catId)[0]\n",
            "                        precision = precisions[:, :, idx, 0, -1]\n",
            "                        precision = precision[precision > -1]\n",
            "                        if precision.size:\n",
            "                            ap = np.mean(precision)\n",
            "                        else:\n",
            "                            ap = float('nan')\n",
            "                        results_per_category.append(\n",
            "                            (f'{nm[\"name\"]}', f'{float(ap):0.3f}'))\n",
            "\n",
            "                    num_columns = min(6, len(results_per_category) * 2)\n",
            "                    results_flatten = list(\n",
            "                        itertools.chain(*results_per_category))\n",
            "                    headers = ['category', 'AP'] * (num_columns // 2)\n",
            "                    results_2d = itertools.zip_longest(*[\n",
            "                        results_flatten[i::num_columns]\n",
            "                        for i in range(num_columns)\n",
            "                    ])\n",
            "                    table_data = [headers]\n",
            "                    table_data += [result for result in results_2d]\n",
            "                    table = AsciiTable(table_data)\n",
            "                    print_log('\\n' + table.table, logger=logger)\n",
            "\n",
            "                if metric_items is None:\n",
            "                    metric_items = [\n",
            "                        'mAP', 'mAP_50', 'mAP_75', 'mAP_s', 'mAP_m', 'mAP_l'\n",
            "                    ]\n",
            "\n",
            "                for metric_item in metric_items:\n",
            "                    key = f'{metric}_{metric_item}'\n",
            "                    val = float(\n",
            "                        f'{cocoEval.stats[coco_metric_names[metric_item]]:.3f}'\n",
            "                    )\n",
            "                    eval_results[key] = val\n",
            "                ap = cocoEval.stats[:6]\n",
            "                eval_results[f'{metric}_mAP_copypaste'] = (\n",
            "                    f'{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} '\n",
            "                    f'{ap[4]:.3f} {ap[5]:.3f}')\n",
            "        if tmp_dir is not None:\n",
            "            tmp_dir.cleanup()\n",
            "        return eval_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mmcv import Config\n",
        "cfg = Config.fromfile('./configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco.py')"
      ],
      "metadata": {
        "id": "Kgj7txUvjd6X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "coS4WEgS_3aE",
        "outputId": "9cb50f85-69ea-4848-94a6-bc2c3932ef03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/mmdetection'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.dataset_type = 'CocoDataset'\n",
        "cfg.classes = ('_background_','back_bumper', 'back_glass', 'back_left_door', 'back_left_light', 'back_right_door', 'back_right_light', 'front_bumper', 'front_glass', 'front_left_door', 'front_left_light', 'front_right_door', 'front_right_light', 'hood', 'left_mirror', 'right_mirror', 'tailgate', 'trunk', 'wheel')\n"
      ],
      "metadata": {
        "id": "PNWPM44Vj3Uj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd content/mmdetection/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bqbW-w0qsdO",
        "outputId": "fc75f4f7-6997-40aa-8882-3c407ae91e97"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'content/mmdetection/'\n",
            "/content/mmdetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mmdet.apis import set_random_seed\n",
        "\n",
        "# Modify dataset type and path\n",
        "PREFIX='content/drive/MyDrive/dataset_custom/'\n",
        "cfg.data.test.ann_file = PREFIX+'testset/annotations.json'\n",
        "cfg.data.test.img_prefix = PREFIX+'testset'\n",
        "cfg.data.test.classes = cfg.classes\n",
        "cfg.data.test.pipeline = [\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(\n",
        "                type='MultiScaleFlipAug',\n",
        "                img_scale=(512, 512),\n",
        "                flip=False,\n",
        "                transforms=[\n",
        "                    dict(type='Resize', keep_ratio=True),\n",
        "                    dict(type='RandomFlip'),\n",
        "                    dict(\n",
        "                        type='Normalize',\n",
        "                        mean=[103.53, 116.28, 123.675],\n",
        "                        std=[1.0, 1.0, 1.0],\n",
        "                        to_rgb=False),\n",
        "                    dict(type='Pad', size_divisor=32),\n",
        "                    dict(type='ImageToTensor', keys=['img']),\n",
        "                    dict(type='Collect', keys=['img'])\n",
        "                ])\n",
        "        ]\n",
        "\n",
        "cfg.data.train.ann_file = PREFIX+'trainingset/annotations.json'\n",
        "cfg.data.train.img_prefix = PREFIX+'trainingset'\n",
        "cfg.data.train.classes = cfg.classes\n",
        "cfg.data.train.pipeline = [\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(\n",
        "                type='LoadAnnotations',\n",
        "                with_bbox=True,\n",
        "                with_mask=True,\n",
        "                poly2mask=False),\n",
        "            dict(\n",
        "                type='Resize',\n",
        "                img_scale=(512, 512),\n",
        "                multiscale_mode='value',\n",
        "                keep_ratio=True),\n",
        "            dict(type='RandomFlip', flip_ratio=0.5),\n",
        "            dict(\n",
        "                type='Normalize',\n",
        "                mean=[103.53, 116.28, 123.675],\n",
        "                std=[1.0, 1.0, 1.0],\n",
        "                to_rgb=False),\n",
        "            dict(type='Pad', size_divisor=32),\n",
        "            dict(type='DefaultFormatBundle'),\n",
        "            dict(\n",
        "                type='Collect',\n",
        "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
        "        ]\n",
        "\n",
        "cfg.data.val.ann_file = PREFIX+'testset/annotations.json'\n",
        "cfg.data.val.img_prefix = PREFIX+'testset'\n",
        "cfg.data.val.classes = cfg.classes\n",
        "cfg.data.val.pipeline =[\n",
        "            dict(type='LoadImageFromFile'),\n",
        "            dict(\n",
        "                type='MultiScaleFlipAug',\n",
        "                img_scale=(512, 512),\n",
        "                flip=False,\n",
        "                transforms=[\n",
        "                    dict(type='Resize', keep_ratio=True),\n",
        "                    dict(type='RandomFlip'),\n",
        "                    dict(\n",
        "                        type='Normalize',\n",
        "                        mean=[103.53, 116.28, 123.675],\n",
        "                        std=[1.0, 1.0, 1.0],\n",
        "                        to_rgb=False),\n",
        "                    dict(type='Pad', size_divisor=32),\n",
        "                    dict(type='ImageToTensor', keys=['img']),\n",
        "                    dict(type='Collect', keys=['img'])\n",
        "                ])\n",
        "        ]\n",
        "\n",
        "# modify num classes of the model in box head and mask head\n",
        "cfg.model.roi_head.bbox_head.num_classes = 19\n",
        "cfg.model.roi_head.mask_head.num_classes = 19\n",
        "\n",
        "\n",
        "# We can still the pre-trained Mask RCNN model to obtain a higher performance\n",
        "cfg.load_from = PREFIX+'pretrained/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
        "\n",
        "# Set up working dir to save files and logs.\n",
        "cfg.work_dir = PREFIX+'checkpoint'\n",
        "\n",
        "# The original learning rate (LR) is set for 8-GPU training.\n",
        "# We divide it by 8 since we only use one GPU.\n",
        "cfg.optimizer.lr = 0.01 / 8\n",
        "cfg.lr_config.warmup = None\n",
        "cfg.log_config.interval = 100\n",
        "cfg.optimizer = dict(type='SGD', lr=0.01/8, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "# We can set the evaluation interval to reduce the evaluation times\n",
        "cfg.evaluation.interval = 12\n",
        "# We can set the checkpoint saving interval to reduce the storage cost\n",
        "cfg.checkpoint_config.interval = 12\n",
        "\n",
        "# Set seed thus the results are more reproducible\n",
        "cfg.seed = 0\n",
        "set_random_seed(0, deterministic=False)\n",
        "cfg.gpu_ids = range(1)\n",
        "\n",
        "# We can also use tensorboard to log the training process\n",
        "cfg.log_config.hooks = [\n",
        "    dict(type='TextLoggerHook'),\n",
        "    dict(type='TensorboardLoggerHook')]\n",
        "\n",
        "cfg.train_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(type='LoadAnnotations', with_bbox=True),\n",
        "    dict(type='Resize', img_scale=(512, 512), keep_ratio=True),\n",
        "    dict(type='RandomFlip', flip_ratio=0.5),\n",
        "    dict(type='Normalize',\n",
        "          mean=[103.53, 116.28, 123.675],\n",
        "          std=[1.0, 1.0, 1.0],\n",
        "          to_rgb=False),\n",
        "    dict(type='Pad', size_divisor=32),\n",
        "    dict(type='DefaultFormatBundle'),\n",
        "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels','gt_masks']),\n",
        "]\n",
        "\n",
        "cfg.test_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(\n",
        "        type='MultiScaleFlipAug',\n",
        "        img_scale=(512, 512),\n",
        "        flip=False,\n",
        "        transforms=[\n",
        "            dict(type='Resize', keep_ratio=True),\n",
        "            dict(type='RandomFlip'),\n",
        "            dict(type='Normalize',\n",
        "              mean=[103.53, 116.28, 123.675],\n",
        "              std=[1.0, 1.0, 1.0],\n",
        "              to_rgb=False),\n",
        "            dict(type='Pad', size_divisor=32),\n",
        "            dict(type='ImageToTensor', keys=['img']),\n",
        "            dict(type='Collect', keys=['img']),\n",
        "        ])\n",
        "]\n",
        "\n",
        "cfg.model.rpn_head.loss_cls=dict(\n",
        "    type='FocalLoss',\n",
        "    use_sigmoid=True,\n",
        "    gamma=2.0,\n",
        "    alpha=0.25,\n",
        "    loss_weight=0.5)\n",
        "\n",
        "lr_config = dict(\n",
        "    policy='step',\n",
        "    warmup=None,\n",
        "    warmup_iters=100,\n",
        "    warmup_ratio=0.001,\n",
        "    step=[8, 11])\n",
        "# We can initialize the logger for training and have a look\n",
        "# at the final config used for training\n",
        "print(f'Config:\\n{cfg.pretty_text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTMFElwIjflK",
        "outputId": "bc4a0793-57e9-4123-9851-511b8de977b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config:\n",
            "model = dict(\n",
            "    type='MaskRCNN',\n",
            "    backbone=dict(\n",
            "        type='ResNet',\n",
            "        depth=50,\n",
            "        num_stages=4,\n",
            "        out_indices=(0, 1, 2, 3),\n",
            "        frozen_stages=1,\n",
            "        norm_cfg=dict(type='BN', requires_grad=False),\n",
            "        norm_eval=True,\n",
            "        style='caffe',\n",
            "        init_cfg=dict(\n",
            "            type='Pretrained',\n",
            "            checkpoint='open-mmlab://detectron2/resnet50_caffe')),\n",
            "    neck=dict(\n",
            "        type='FPN',\n",
            "        in_channels=[256, 512, 1024, 2048],\n",
            "        out_channels=256,\n",
            "        num_outs=5),\n",
            "    rpn_head=dict(\n",
            "        type='RPNHead',\n",
            "        in_channels=256,\n",
            "        feat_channels=256,\n",
            "        anchor_generator=dict(\n",
            "            type='AnchorGenerator',\n",
            "            scales=[8],\n",
            "            ratios=[0.5, 1.0, 2.0],\n",
            "            strides=[4, 8, 16, 32, 64]),\n",
            "        bbox_coder=dict(\n",
            "            type='DeltaXYWHBBoxCoder',\n",
            "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
            "        loss_cls=dict(\n",
            "            type='FocalLoss',\n",
            "            use_sigmoid=True,\n",
            "            gamma=2.0,\n",
            "            alpha=0.25,\n",
            "            loss_weight=0.5),\n",
            "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
            "    roi_head=dict(\n",
            "        type='StandardRoIHead',\n",
            "        bbox_roi_extractor=dict(\n",
            "            type='SingleRoIExtractor',\n",
            "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
            "            out_channels=256,\n",
            "            featmap_strides=[4, 8, 16, 32]),\n",
            "        bbox_head=dict(\n",
            "            type='Shared2FCBBoxHead',\n",
            "            in_channels=256,\n",
            "            fc_out_channels=1024,\n",
            "            roi_feat_size=7,\n",
            "            num_classes=19,\n",
            "            bbox_coder=dict(\n",
            "                type='DeltaXYWHBBoxCoder',\n",
            "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
            "            reg_class_agnostic=False,\n",
            "            loss_cls=dict(\n",
            "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
            "            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
            "        mask_roi_extractor=dict(\n",
            "            type='SingleRoIExtractor',\n",
            "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
            "            out_channels=256,\n",
            "            featmap_strides=[4, 8, 16, 32]),\n",
            "        mask_head=dict(\n",
            "            type='FCNMaskHead',\n",
            "            num_convs=4,\n",
            "            in_channels=256,\n",
            "            conv_out_channels=256,\n",
            "            num_classes=19,\n",
            "            loss_mask=dict(\n",
            "                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),\n",
            "    train_cfg=dict(\n",
            "        rpn=dict(\n",
            "            assigner=dict(\n",
            "                type='MaxIoUAssigner',\n",
            "                pos_iou_thr=0.7,\n",
            "                neg_iou_thr=0.3,\n",
            "                min_pos_iou=0.3,\n",
            "                match_low_quality=True,\n",
            "                ignore_iof_thr=-1),\n",
            "            sampler=dict(\n",
            "                type='RandomSampler',\n",
            "                num=256,\n",
            "                pos_fraction=0.5,\n",
            "                neg_pos_ub=-1,\n",
            "                add_gt_as_proposals=False),\n",
            "            allowed_border=-1,\n",
            "            pos_weight=-1,\n",
            "            debug=False),\n",
            "        rpn_proposal=dict(\n",
            "            nms_pre=2000,\n",
            "            max_per_img=1000,\n",
            "            nms=dict(type='nms', iou_threshold=0.7),\n",
            "            min_bbox_size=0),\n",
            "        rcnn=dict(\n",
            "            assigner=dict(\n",
            "                type='MaxIoUAssigner',\n",
            "                pos_iou_thr=0.5,\n",
            "                neg_iou_thr=0.5,\n",
            "                min_pos_iou=0.5,\n",
            "                match_low_quality=True,\n",
            "                ignore_iof_thr=-1),\n",
            "            sampler=dict(\n",
            "                type='RandomSampler',\n",
            "                num=512,\n",
            "                pos_fraction=0.25,\n",
            "                neg_pos_ub=-1,\n",
            "                add_gt_as_proposals=True),\n",
            "            mask_size=28,\n",
            "            pos_weight=-1,\n",
            "            debug=False)),\n",
            "    test_cfg=dict(\n",
            "        rpn=dict(\n",
            "            nms_pre=1000,\n",
            "            max_per_img=1000,\n",
            "            nms=dict(type='nms', iou_threshold=0.7),\n",
            "            min_bbox_size=0),\n",
            "        rcnn=dict(\n",
            "            score_thr=0.05,\n",
            "            nms=dict(type='nms', iou_threshold=0.5),\n",
            "            max_per_img=100,\n",
            "            mask_thr_binary=0.5)))\n",
            "dataset_type = 'CocoDataset'\n",
            "data_root = 'data/coco/'\n",
            "img_norm_cfg = dict(\n",
            "    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\n",
            "train_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(type='LoadAnnotations', with_bbox=True),\n",
            "    dict(type='Resize', img_scale=(512, 512), keep_ratio=True),\n",
            "    dict(type='RandomFlip', flip_ratio=0.5),\n",
            "    dict(\n",
            "        type='Normalize',\n",
            "        mean=[103.53, 116.28, 123.675],\n",
            "        std=[1.0, 1.0, 1.0],\n",
            "        to_rgb=False),\n",
            "    dict(type='Pad', size_divisor=32),\n",
            "    dict(type='DefaultFormatBundle'),\n",
            "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='MultiScaleFlipAug',\n",
            "        img_scale=(512, 512),\n",
            "        flip=False,\n",
            "        transforms=[\n",
            "            dict(type='Resize', keep_ratio=True),\n",
            "            dict(type='RandomFlip'),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[103.53, 116.28, 123.675],\n",
            "                std=[1.0, 1.0, 1.0],\n",
            "                to_rgb=False),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='ImageToTensor', keys=['img']),\n",
            "            dict(type='Collect', keys=['img'])\n",
            "        ])\n",
            "]\n",
            "data = dict(\n",
            "    samples_per_gpu=2,\n",
            "    workers_per_gpu=2,\n",
            "    train=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file=\n",
            "        'content/drive/MyDrive/dataset_custom/trainingset/annotations.json',\n",
            "        img_prefix='content/drive/MyDrive/dataset_custom/trainingset',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='LoadAnnotations',\n",
            "                with_bbox=True,\n",
            "                with_mask=True,\n",
            "                poly2mask=False),\n",
            "            dict(\n",
            "                type='Resize',\n",
            "                img_scale=(512, 512),\n",
            "                multiscale_mode='value',\n",
            "                keep_ratio=True),\n",
            "            dict(type='RandomFlip', flip_ratio=0.5),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[103.53, 116.28, 123.675],\n",
            "                std=[1.0, 1.0, 1.0],\n",
            "                to_rgb=False),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='DefaultFormatBundle'),\n",
            "            dict(\n",
            "                type='Collect',\n",
            "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
            "        ],\n",
            "        classes=('_background_', 'back_bumper', 'back_glass', 'back_left_door',\n",
            "                 'back_left_light', 'back_right_door', 'back_right_light',\n",
            "                 'front_bumper', 'front_glass', 'front_left_door',\n",
            "                 'front_left_light', 'front_right_door', 'front_right_light',\n",
            "                 'hood', 'left_mirror', 'right_mirror', 'tailgate', 'trunk',\n",
            "                 'wheel')),\n",
            "    val=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file=\n",
            "        'content/drive/MyDrive/dataset_custom/testset/annotations.json',\n",
            "        img_prefix='content/drive/MyDrive/dataset_custom/testset',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(512, 512),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[103.53, 116.28, 123.675],\n",
            "                        std=[1.0, 1.0, 1.0],\n",
            "                        to_rgb=False),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ],\n",
            "        classes=('_background_', 'back_bumper', 'back_glass', 'back_left_door',\n",
            "                 'back_left_light', 'back_right_door', 'back_right_light',\n",
            "                 'front_bumper', 'front_glass', 'front_left_door',\n",
            "                 'front_left_light', 'front_right_door', 'front_right_light',\n",
            "                 'hood', 'left_mirror', 'right_mirror', 'tailgate', 'trunk',\n",
            "                 'wheel')),\n",
            "    test=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file=\n",
            "        'content/drive/MyDrive/dataset_custom/testset/annotations.json',\n",
            "        img_prefix='content/drive/MyDrive/dataset_custom/testset',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(512, 512),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[103.53, 116.28, 123.675],\n",
            "                        std=[1.0, 1.0, 1.0],\n",
            "                        to_rgb=False),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ],\n",
            "        classes=('_background_', 'back_bumper', 'back_glass', 'back_left_door',\n",
            "                 'back_left_light', 'back_right_door', 'back_right_light',\n",
            "                 'front_bumper', 'front_glass', 'front_left_door',\n",
            "                 'front_left_light', 'front_right_door', 'front_right_light',\n",
            "                 'hood', 'left_mirror', 'right_mirror', 'tailgate', 'trunk',\n",
            "                 'wheel')))\n",
            "evaluation = dict(metric=['bbox', 'segm'], interval=12)\n",
            "optimizer = dict(type='SGD', lr=0.00125, momentum=0.9, weight_decay=0.0001)\n",
            "optimizer_config = dict(grad_clip=None)\n",
            "lr_config = dict(\n",
            "    policy='step',\n",
            "    warmup=None,\n",
            "    warmup_iters=500,\n",
            "    warmup_ratio=0.001,\n",
            "    step=[28, 34])\n",
            "runner = dict(type='EpochBasedRunner', max_epochs=36)\n",
            "checkpoint_config = dict(interval=12)\n",
            "log_config = dict(\n",
            "    interval=100,\n",
            "    hooks=[dict(type='TextLoggerHook'),\n",
            "           dict(type='TensorboardLoggerHook')])\n",
            "custom_hooks = [dict(type='NumClassCheckHook')]\n",
            "dist_params = dict(backend='nccl')\n",
            "log_level = 'INFO'\n",
            "load_from = 'content/drive/MyDrive/dataset_custom/pretrained/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
            "resume_from = None\n",
            "workflow = [('train', 1)]\n",
            "opencv_num_threads = 0\n",
            "mp_start_method = 'fork'\n",
            "classes = ('_background_', 'back_bumper', 'back_glass', 'back_left_door',\n",
            "           'back_left_light', 'back_right_door', 'back_right_light',\n",
            "           'front_bumper', 'front_glass', 'front_left_door',\n",
            "           'front_left_light', 'front_right_door', 'front_right_light', 'hood',\n",
            "           'left_mirror', 'right_mirror', 'tailgate', 'trunk', 'wheel')\n",
            "work_dir = 'content/drive/MyDrive/dataset_custom/checkpoint'\n",
            "seed = 0\n",
            "gpu_ids = range(0, 1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glD_355jDpcb",
        "outputId": "c5c898bc-eb75-4a95-ac4c-66b3a82007b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mmdet.datasets import build_dataset\n",
        "from mmdet.models import build_detector\n",
        "from mmdet.apis import train_detector\n",
        "import mmcv\n",
        "import os.path as osp\n",
        "\n",
        "\n",
        "# Build dataset\n",
        "datasets = [build_dataset(cfg.data.train)]\n",
        "\n",
        "# Build the detector\n",
        "model = build_detector(cfg.model)\n",
        "\n",
        "# Add an attribute for visualization convenience\n",
        "model.CLASSES = datasets[0].CLASSES\n",
        "\n",
        "# Create work_dir\n",
        "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
        "train_detector(model, datasets, cfg, distributed=False, validate=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMGXRsfno-ue",
        "outputId": "388cbf3b-2e8e-44c1-d07d-c13837cd3907"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.58s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mmdetection/mmdet/models/dense_heads/anchor_head.py:94: UserWarning: DeprecationWarning: Determining whether to samplingby loss type is deprecated, please delete sampler inyour config when using `FocalLoss`, `GHMC`, `QualityFocalLoss` or other FocalLoss variant.\n",
            "  'DeprecationWarning: Determining whether to sampling'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-17 23:33:22,328 - mmdet - INFO - load checkpoint from local path: content/drive/MyDrive/dataset_custom/pretrained/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done (t=0.32s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-17 23:33:25,725 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for roi_head.bbox_head.fc_cls.weight: copying a param with shape torch.Size([81, 1024]) from checkpoint, the shape in current model is torch.Size([20, 1024]).\n",
            "size mismatch for roi_head.bbox_head.fc_cls.bias: copying a param with shape torch.Size([81]) from checkpoint, the shape in current model is torch.Size([20]).\n",
            "size mismatch for roi_head.bbox_head.fc_reg.weight: copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape in current model is torch.Size([76, 1024]).\n",
            "size mismatch for roi_head.bbox_head.fc_reg.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([76]).\n",
            "size mismatch for roi_head.mask_head.conv_logits.weight: copying a param with shape torch.Size([80, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([19, 256, 1, 1]).\n",
            "size mismatch for roi_head.mask_head.conv_logits.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([19]).\n",
            "2022-03-17 23:33:25,737 - mmdet - INFO - Start running, host: root@93d9ff578770, work_dir: /content/drive/MyDrive/dataset_custom/checkpoint\n",
            "2022-03-17 23:33:25,739 - mmdet - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            "(VERY_LOW    ) TensorboardLoggerHook              \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
            "(NORMAL      ) NumClassCheckHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            "(VERY_LOW    ) TensorboardLoggerHook              \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(ABOVE_NORMAL) OptimizerHook                      \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            "(VERY_LOW    ) TensorboardLoggerHook              \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            "(VERY_LOW    ) TensorboardLoggerHook              \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(NORMAL      ) NumClassCheckHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            "(VERY_LOW    ) TensorboardLoggerHook              \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            "(VERY_LOW    ) TensorboardLoggerHook              \n",
            " -------------------- \n",
            "after_run:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            "(VERY_LOW    ) TensorboardLoggerHook              \n",
            " -------------------- \n",
            "2022-03-17 23:33:25,745 - mmdet - INFO - workflow: [('train', 1)], max: 36 epochs\n",
            "2022-03-17 23:33:25,747 - mmdet - INFO - Checkpoints will be saved to /content/drive/MyDrive/dataset_custom/checkpoint by HardDiskBackend.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "2022-03-17 23:34:54,808 - mmdet - INFO - Epoch [1][100/200]\tlr: 1.250e-03, eta: 1:36:46, time: 0.818, data_time: 0.033, memory: 1815, loss_rpn_cls: 0.3896, loss_rpn_bbox: 0.4984, loss_cls: 0.7274, acc: 84.8320, loss_bbox: 0.4886, loss_mask: 0.5795, loss: 2.6834\n",
            "2022-03-17 23:36:17,723 - mmdet - INFO - Epoch [1][200/200]\tlr: 1.250e-03, eta: 1:36:04, time: 0.829, data_time: 0.011, memory: 1958, loss_rpn_cls: 0.1391, loss_rpn_bbox: 0.3849, loss_cls: 0.6301, acc: 83.3242, loss_bbox: 0.5822, loss_mask: 0.2950, loss: 2.0314\n",
            "2022-03-17 23:37:46,124 - mmdet - INFO - Epoch [2][100/200]\tlr: 1.250e-03, eta: 1:36:52, time: 0.880, data_time: 0.030, memory: 1971, loss_rpn_cls: 0.0982, loss_rpn_bbox: 0.3193, loss_cls: 0.5012, acc: 84.6445, loss_bbox: 0.4905, loss_mask: 0.2224, loss: 1.6317\n",
            "2022-03-17 23:39:11,804 - mmdet - INFO - Epoch [2][200/200]\tlr: 1.250e-03, eta: 1:35:53, time: 0.857, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.1031, loss_rpn_bbox: 0.3348, loss_cls: 0.4213, acc: 85.8330, loss_bbox: 0.4107, loss_mask: 0.2073, loss: 1.4773\n",
            "2022-03-17 23:40:40,964 - mmdet - INFO - Epoch [3][100/200]\tlr: 1.250e-03, eta: 1:35:24, time: 0.888, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0716, loss_rpn_bbox: 0.3015, loss_cls: 0.3535, acc: 87.3506, loss_bbox: 0.3542, loss_mask: 0.1794, loss: 1.2602\n",
            "2022-03-17 23:42:08,678 - mmdet - INFO - Epoch [3][200/200]\tlr: 1.250e-03, eta: 1:34:23, time: 0.877, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0737, loss_rpn_bbox: 0.2959, loss_cls: 0.3394, acc: 87.2412, loss_bbox: 0.3352, loss_mask: 0.1671, loss: 1.2112\n",
            "2022-03-17 23:43:38,590 - mmdet - INFO - Epoch [4][100/200]\tlr: 1.250e-03, eta: 1:33:32, time: 0.895, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0564, loss_rpn_bbox: 0.2655, loss_cls: 0.3026, acc: 88.2178, loss_bbox: 0.3001, loss_mask: 0.1583, loss: 1.0829\n",
            "2022-03-17 23:45:07,079 - mmdet - INFO - Epoch [4][200/200]\tlr: 1.250e-03, eta: 1:32:23, time: 0.885, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0540, loss_rpn_bbox: 0.2588, loss_cls: 0.2978, acc: 88.5039, loss_bbox: 0.2936, loss_mask: 0.1554, loss: 1.0595\n",
            "2022-03-17 23:46:37,749 - mmdet - INFO - Epoch [5][100/200]\tlr: 1.250e-03, eta: 1:31:22, time: 0.903, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0410, loss_rpn_bbox: 0.2418, loss_cls: 0.2626, acc: 89.3809, loss_bbox: 0.2665, loss_mask: 0.1409, loss: 0.9529\n",
            "2022-03-17 23:48:06,381 - mmdet - INFO - Epoch [5][200/200]\tlr: 1.250e-03, eta: 1:30:05, time: 0.886, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0464, loss_rpn_bbox: 0.2414, loss_cls: 0.2711, acc: 89.2471, loss_bbox: 0.2680, loss_mask: 0.1498, loss: 0.9766\n",
            "2022-03-17 23:49:38,143 - mmdet - INFO - Epoch [6][100/200]\tlr: 1.250e-03, eta: 1:29:01, time: 0.914, data_time: 0.030, memory: 1971, loss_rpn_cls: 0.0356, loss_rpn_bbox: 0.2346, loss_cls: 0.2531, acc: 90.0615, loss_bbox: 0.2504, loss_mask: 0.1405, loss: 0.9141\n",
            "2022-03-17 23:51:06,593 - mmdet - INFO - Epoch [6][200/200]\tlr: 1.250e-03, eta: 1:27:38, time: 0.885, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0358, loss_rpn_bbox: 0.2199, loss_cls: 0.2386, acc: 90.0830, loss_bbox: 0.2411, loss_mask: 0.1327, loss: 0.8680\n",
            "2022-03-17 23:52:38,496 - mmdet - INFO - Epoch [7][100/200]\tlr: 1.250e-03, eta: 1:26:27, time: 0.915, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0277, loss_rpn_bbox: 0.2142, loss_cls: 0.2265, acc: 90.6914, loss_bbox: 0.2282, loss_mask: 0.1311, loss: 0.8277\n",
            "2022-03-17 23:54:08,480 - mmdet - INFO - Epoch [7][200/200]\tlr: 1.250e-03, eta: 1:25:08, time: 0.900, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0268, loss_rpn_bbox: 0.2059, loss_cls: 0.2229, acc: 90.8604, loss_bbox: 0.2328, loss_mask: 0.1282, loss: 0.8166\n",
            "2022-03-17 23:55:40,587 - mmdet - INFO - Epoch [8][100/200]\tlr: 1.250e-03, eta: 1:23:54, time: 0.917, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0262, loss_rpn_bbox: 0.2156, loss_cls: 0.2103, acc: 91.4795, loss_bbox: 0.2138, loss_mask: 0.1213, loss: 0.7873\n",
            "2022-03-17 23:57:10,113 - mmdet - INFO - Epoch [8][200/200]\tlr: 1.250e-03, eta: 1:22:30, time: 0.895, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0247, loss_rpn_bbox: 0.2059, loss_cls: 0.2058, acc: 91.5459, loss_bbox: 0.2115, loss_mask: 0.1234, loss: 0.7713\n",
            "2022-03-17 23:58:42,214 - mmdet - INFO - Epoch [9][100/200]\tlr: 1.250e-03, eta: 1:21:12, time: 0.917, data_time: 0.032, memory: 1971, loss_rpn_cls: 0.0207, loss_rpn_bbox: 0.1912, loss_cls: 0.1984, acc: 91.9053, loss_bbox: 0.2065, loss_mask: 0.1232, loss: 0.7400\n",
            "2022-03-18 00:00:11,614 - mmdet - INFO - Epoch [9][200/200]\tlr: 1.250e-03, eta: 1:19:46, time: 0.894, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0214, loss_rpn_bbox: 0.2029, loss_cls: 0.2015, acc: 91.4756, loss_bbox: 0.2016, loss_mask: 0.1175, loss: 0.7449\n",
            "2022-03-18 00:01:43,887 - mmdet - INFO - Epoch [10][100/200]\tlr: 1.250e-03, eta: 1:18:26, time: 0.919, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0195, loss_rpn_bbox: 0.1889, loss_cls: 0.1941, acc: 92.0381, loss_bbox: 0.1977, loss_mask: 0.1191, loss: 0.7192\n",
            "2022-03-18 00:03:13,509 - mmdet - INFO - Epoch [10][200/200]\tlr: 1.250e-03, eta: 1:16:59, time: 0.896, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0180, loss_rpn_bbox: 0.1879, loss_cls: 0.1882, acc: 91.9541, loss_bbox: 0.1915, loss_mask: 0.1145, loss: 0.7001\n",
            "2022-03-18 00:04:45,096 - mmdet - INFO - Epoch [11][100/200]\tlr: 1.250e-03, eta: 1:15:36, time: 0.912, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0144, loss_rpn_bbox: 0.1846, loss_cls: 0.1734, acc: 92.7539, loss_bbox: 0.1786, loss_mask: 0.1155, loss: 0.6665\n",
            "2022-03-18 00:06:15,661 - mmdet - INFO - Epoch [11][200/200]\tlr: 1.250e-03, eta: 1:14:11, time: 0.906, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0150, loss_rpn_bbox: 0.1773, loss_cls: 0.1784, acc: 92.5518, loss_bbox: 0.1843, loss_mask: 0.1113, loss: 0.6663\n",
            "2022-03-18 00:07:48,161 - mmdet - INFO - Epoch [12][100/200]\tlr: 1.250e-03, eta: 1:12:48, time: 0.921, data_time: 0.030, memory: 1971, loss_rpn_cls: 0.0152, loss_rpn_bbox: 0.1753, loss_cls: 0.1741, acc: 92.6318, loss_bbox: 0.1829, loss_mask: 0.1121, loss: 0.6597\n",
            "2022-03-18 00:09:18,118 - mmdet - INFO - Epoch [12][200/200]\tlr: 1.250e-03, eta: 1:11:21, time: 0.900, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0120, loss_rpn_bbox: 0.1752, loss_cls: 0.1658, acc: 93.1963, loss_bbox: 0.1735, loss_mask: 0.1070, loss: 0.6334\n",
            "2022-03-18 00:09:18,206 - mmdet - INFO - Saving checkpoint at 12 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 5.0 task/s, elapsed: 20s, ETA:     0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-18 00:09:40,958 - mmdet - INFO - Evaluating bbox...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.44s).\n",
            "Accumulating evaluation results...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-18 00:09:41,646 - mmdet - INFO - \n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.396\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.585\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.449\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.272\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.386\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.428\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.603\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.603\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.603\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.391\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.542\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.658\n",
            "\n",
            "2022-03-18 00:09:41,651 - mmdet - INFO - Evaluating segm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE (t=0.23s).\n",
            "Loading and preparing results...\n",
            "DONE (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mmdetection/mmdet/datasets/coco.py:465: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
            "  UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE (t=0.79s).\n",
            "Accumulating evaluation results...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pycocotools/cocoeval.py:378: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n",
            "2022-03-18 00:09:42,748 - mmdet - INFO - \n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.421\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.598\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.487\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.392\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.629\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.629\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.629\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.495\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.569\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.683\n",
            "\n",
            "2022-03-18 00:09:42,760 - mmdet - INFO - Epoch(val) [12][100]\tbbox_mAP: 0.3960, bbox_mAP_50: 0.5850, bbox_mAP_75: 0.4490, bbox_mAP_s: 0.2720, bbox_mAP_m: 0.3860, bbox_mAP_l: 0.4280, bbox_mAP_copypaste: 0.396 0.585 0.449 0.272 0.386 0.428, segm_mAP: 0.4210, segm_mAP_50: 0.5980, segm_mAP_75: 0.4870, segm_mAP_s: 0.2410, segm_mAP_m: 0.3920, segm_mAP_l: 0.4850, segm_mAP_copypaste: 0.421 0.598 0.487 0.241 0.392 0.485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE (t=0.24s).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mmdetection/mmdet/core/mask/structures.py:1071: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  bitmap_mask = maskUtils.decode(rle).astype(np.bool)\n",
            "2022-03-18 00:11:14,822 - mmdet - INFO - Epoch [13][100/200]\tlr: 1.250e-03, eta: 1:09:56, time: 0.917, data_time: 0.032, memory: 1971, loss_rpn_cls: 0.0116, loss_rpn_bbox: 0.1727, loss_cls: 0.1592, acc: 93.3262, loss_bbox: 0.1670, loss_mask: 0.1084, loss: 0.6190\n",
            "2022-03-18 00:12:44,544 - mmdet - INFO - Epoch [13][200/200]\tlr: 1.250e-03, eta: 1:08:28, time: 0.897, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0119, loss_rpn_bbox: 0.1656, loss_cls: 0.1579, acc: 93.4453, loss_bbox: 0.1725, loss_mask: 0.1074, loss: 0.6152\n",
            "2022-03-18 00:14:17,065 - mmdet - INFO - Epoch [14][100/200]\tlr: 1.250e-03, eta: 1:07:03, time: 0.921, data_time: 0.032, memory: 1971, loss_rpn_cls: 0.0110, loss_rpn_bbox: 0.1610, loss_cls: 0.1579, acc: 93.4844, loss_bbox: 0.1672, loss_mask: 0.1063, loss: 0.6033\n",
            "2022-03-18 00:15:47,154 - mmdet - INFO - Epoch [14][200/200]\tlr: 1.250e-03, eta: 1:05:35, time: 0.901, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0096, loss_rpn_bbox: 0.1685, loss_cls: 0.1448, acc: 93.9814, loss_bbox: 0.1571, loss_mask: 0.1022, loss: 0.5823\n",
            "2022-03-18 00:17:19,624 - mmdet - INFO - Epoch [15][100/200]\tlr: 1.250e-03, eta: 1:04:09, time: 0.920, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0100, loss_rpn_bbox: 0.1596, loss_cls: 0.1417, acc: 94.0332, loss_bbox: 0.1549, loss_mask: 0.1015, loss: 0.5677\n",
            "2022-03-18 00:18:49,977 - mmdet - INFO - Epoch [15][200/200]\tlr: 1.250e-03, eta: 1:02:41, time: 0.904, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0094, loss_rpn_bbox: 0.1550, loss_cls: 0.1471, acc: 93.8838, loss_bbox: 0.1620, loss_mask: 0.1033, loss: 0.5768\n",
            "2022-03-18 00:20:22,940 - mmdet - INFO - Epoch [16][100/200]\tlr: 1.250e-03, eta: 1:01:15, time: 0.925, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0094, loss_rpn_bbox: 0.1586, loss_cls: 0.1338, acc: 94.4766, loss_bbox: 0.1534, loss_mask: 0.1001, loss: 0.5552\n",
            "2022-03-18 00:21:54,058 - mmdet - INFO - Epoch [16][200/200]\tlr: 1.250e-03, eta: 0:59:47, time: 0.911, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0082, loss_rpn_bbox: 0.1536, loss_cls: 0.1415, acc: 94.0986, loss_bbox: 0.1577, loss_mask: 0.1009, loss: 0.5619\n",
            "2022-03-18 00:23:27,420 - mmdet - INFO - Epoch [17][100/200]\tlr: 1.250e-03, eta: 0:58:22, time: 0.929, data_time: 0.032, memory: 1971, loss_rpn_cls: 0.0068, loss_rpn_bbox: 0.1529, loss_cls: 0.1306, acc: 94.6221, loss_bbox: 0.1499, loss_mask: 0.0997, loss: 0.5399\n",
            "2022-03-18 00:24:57,809 - mmdet - INFO - Epoch [17][200/200]\tlr: 1.250e-03, eta: 0:56:52, time: 0.904, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0067, loss_rpn_bbox: 0.1477, loss_cls: 0.1277, acc: 94.7529, loss_bbox: 0.1480, loss_mask: 0.0980, loss: 0.5280\n",
            "2022-03-18 00:26:31,026 - mmdet - INFO - Epoch [18][100/200]\tlr: 1.250e-03, eta: 0:55:26, time: 0.928, data_time: 0.032, memory: 1971, loss_rpn_cls: 0.0067, loss_rpn_bbox: 0.1434, loss_cls: 0.1238, acc: 94.8877, loss_bbox: 0.1455, loss_mask: 0.0988, loss: 0.5182\n",
            "2022-03-18 00:28:01,035 - mmdet - INFO - Epoch [18][200/200]\tlr: 1.250e-03, eta: 0:53:56, time: 0.900, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0060, loss_rpn_bbox: 0.1568, loss_cls: 0.1214, acc: 95.1494, loss_bbox: 0.1428, loss_mask: 0.0963, loss: 0.5233\n",
            "2022-03-18 00:29:33,537 - mmdet - INFO - Epoch [19][100/200]\tlr: 1.250e-03, eta: 0:52:28, time: 0.921, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0071, loss_rpn_bbox: 0.1453, loss_cls: 0.1166, acc: 95.4883, loss_bbox: 0.1428, loss_mask: 0.0969, loss: 0.5087\n",
            "2022-03-18 00:31:04,234 - mmdet - INFO - Epoch [19][200/200]\tlr: 1.250e-03, eta: 0:50:59, time: 0.907, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0058, loss_rpn_bbox: 0.1412, loss_cls: 0.1159, acc: 95.2656, loss_bbox: 0.1396, loss_mask: 0.0950, loss: 0.4975\n",
            "2022-03-18 00:32:36,681 - mmdet - INFO - Epoch [20][100/200]\tlr: 1.250e-03, eta: 0:49:31, time: 0.920, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0057, loss_rpn_bbox: 0.1483, loss_cls: 0.1103, acc: 95.6299, loss_bbox: 0.1368, loss_mask: 0.0944, loss: 0.4954\n",
            "2022-03-18 00:34:06,726 - mmdet - INFO - Epoch [20][200/200]\tlr: 1.250e-03, eta: 0:48:01, time: 0.901, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0054, loss_rpn_bbox: 0.1461, loss_cls: 0.1101, acc: 95.5977, loss_bbox: 0.1360, loss_mask: 0.0944, loss: 0.4920\n",
            "2022-03-18 00:35:38,764 - mmdet - INFO - Epoch [21][100/200]\tlr: 1.250e-03, eta: 0:46:32, time: 0.916, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0051, loss_rpn_bbox: 0.1401, loss_cls: 0.1036, acc: 96.1240, loss_bbox: 0.1348, loss_mask: 0.0952, loss: 0.4789\n",
            "2022-03-18 00:37:08,687 - mmdet - INFO - Epoch [21][200/200]\tlr: 1.250e-03, eta: 0:45:02, time: 0.899, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0053, loss_rpn_bbox: 0.1510, loss_cls: 0.1017, acc: 96.0215, loss_bbox: 0.1311, loss_mask: 0.0930, loss: 0.4821\n",
            "2022-03-18 00:38:41,378 - mmdet - INFO - Epoch [22][100/200]\tlr: 1.250e-03, eta: 0:43:33, time: 0.923, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0053, loss_rpn_bbox: 0.1424, loss_cls: 0.0980, acc: 96.3691, loss_bbox: 0.1295, loss_mask: 0.0933, loss: 0.4685\n",
            "2022-03-18 00:40:11,026 - mmdet - INFO - Epoch [22][200/200]\tlr: 1.250e-03, eta: 0:42:03, time: 0.896, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0048, loss_rpn_bbox: 0.1365, loss_cls: 0.0924, acc: 96.5283, loss_bbox: 0.1273, loss_mask: 0.0923, loss: 0.4533\n",
            "2022-03-18 00:41:43,017 - mmdet - INFO - Epoch [23][100/200]\tlr: 1.250e-03, eta: 0:40:33, time: 0.916, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0043, loss_rpn_bbox: 0.1372, loss_cls: 0.0923, acc: 96.5361, loss_bbox: 0.1261, loss_mask: 0.0924, loss: 0.4522\n",
            "2022-03-18 00:43:13,069 - mmdet - INFO - Epoch [23][200/200]\tlr: 1.250e-03, eta: 0:39:03, time: 0.901, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0057, loss_rpn_bbox: 0.1372, loss_cls: 0.0903, acc: 96.5869, loss_bbox: 0.1285, loss_mask: 0.0907, loss: 0.4524\n",
            "2022-03-18 00:44:45,431 - mmdet - INFO - Epoch [24][100/200]\tlr: 1.250e-03, eta: 0:37:34, time: 0.919, data_time: 0.032, memory: 1971, loss_rpn_cls: 0.0045, loss_rpn_bbox: 0.1336, loss_cls: 0.0824, acc: 97.0293, loss_bbox: 0.1228, loss_mask: 0.0901, loss: 0.4335\n",
            "2022-03-18 00:46:15,644 - mmdet - INFO - Epoch [24][200/200]\tlr: 1.250e-03, eta: 0:36:04, time: 0.902, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0039, loss_rpn_bbox: 0.1254, loss_cls: 0.0850, acc: 96.6943, loss_bbox: 0.1245, loss_mask: 0.0893, loss: 0.4282\n",
            "2022-03-18 00:46:15,753 - mmdet - INFO - Saving checkpoint at 24 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 5.3 task/s, elapsed: 19s, ETA:     0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-18 00:46:36,808 - mmdet - INFO - Evaluating bbox...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.45s).\n",
            "Accumulating evaluation results...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-18 00:46:37,490 - mmdet - INFO - \n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.352\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.514\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.393\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.247\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.375\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.386\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.568\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.568\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.568\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.323\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.551\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.607\n",
            "\n",
            "2022-03-18 00:46:37,491 - mmdet - INFO - Evaluating segm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE (t=0.21s).\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mmdetection/mmdet/datasets/coco.py:465: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
            "  UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE (t=0.47s).\n",
            "Accumulating evaluation results...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pycocotools/cocoeval.py:378: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n",
            "2022-03-18 00:46:38,261 - mmdet - INFO - \n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.376\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.529\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.423\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.188\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.371\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.474\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.596\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.596\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.596\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.340\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.581\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.639\n",
            "\n",
            "2022-03-18 00:46:38,275 - mmdet - INFO - Epoch(val) [24][100]\tbbox_mAP: 0.3520, bbox_mAP_50: 0.5140, bbox_mAP_75: 0.3930, bbox_mAP_s: 0.2470, bbox_mAP_m: 0.3750, bbox_mAP_l: 0.3860, bbox_mAP_copypaste: 0.352 0.514 0.393 0.247 0.375 0.386, segm_mAP: 0.3760, segm_mAP_50: 0.5290, segm_mAP_75: 0.4230, segm_mAP_s: 0.1880, segm_mAP_m: 0.3710, segm_mAP_l: 0.4740, segm_mAP_copypaste: 0.376 0.529 0.423 0.188 0.371 0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE (t=0.24s).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mmdetection/mmdet/core/mask/structures.py:1071: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  bitmap_mask = maskUtils.decode(rle).astype(np.bool)\n",
            "2022-03-18 00:48:10,524 - mmdet - INFO - Epoch [25][100/200]\tlr: 1.250e-03, eta: 0:34:34, time: 0.919, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0042, loss_rpn_bbox: 0.1299, loss_cls: 0.0809, acc: 96.9531, loss_bbox: 0.1190, loss_mask: 0.0901, loss: 0.4240\n",
            "2022-03-18 00:49:40,389 - mmdet - INFO - Epoch [25][200/200]\tlr: 1.250e-03, eta: 0:33:04, time: 0.899, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0043, loss_rpn_bbox: 0.1305, loss_cls: 0.0766, acc: 97.2998, loss_bbox: 0.1209, loss_mask: 0.0888, loss: 0.4212\n",
            "2022-03-18 00:51:12,671 - mmdet - INFO - Epoch [26][100/200]\tlr: 1.250e-03, eta: 0:31:34, time: 0.918, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0046, loss_rpn_bbox: 0.1314, loss_cls: 0.0714, acc: 97.5225, loss_bbox: 0.1200, loss_mask: 0.0890, loss: 0.4165\n",
            "2022-03-18 00:52:42,565 - mmdet - INFO - Epoch [26][200/200]\tlr: 1.250e-03, eta: 0:30:04, time: 0.899, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0039, loss_rpn_bbox: 0.1309, loss_cls: 0.0735, acc: 97.2998, loss_bbox: 0.1195, loss_mask: 0.0878, loss: 0.4156\n",
            "2022-03-18 00:54:15,188 - mmdet - INFO - Epoch [27][100/200]\tlr: 1.250e-03, eta: 0:28:35, time: 0.922, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0043, loss_rpn_bbox: 0.1293, loss_cls: 0.0710, acc: 97.4346, loss_bbox: 0.1210, loss_mask: 0.0874, loss: 0.4130\n",
            "2022-03-18 00:55:45,346 - mmdet - INFO - Epoch [27][200/200]\tlr: 1.250e-03, eta: 0:27:04, time: 0.902, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0042, loss_rpn_bbox: 0.1322, loss_cls: 0.0729, acc: 97.3906, loss_bbox: 0.1177, loss_mask: 0.0869, loss: 0.4139\n",
            "2022-03-18 00:57:17,671 - mmdet - INFO - Epoch [28][100/200]\tlr: 1.250e-03, eta: 0:25:35, time: 0.919, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0037, loss_rpn_bbox: 0.1268, loss_cls: 0.0694, acc: 97.4902, loss_bbox: 0.1143, loss_mask: 0.0868, loss: 0.4009\n",
            "2022-03-18 00:58:48,249 - mmdet - INFO - Epoch [28][200/200]\tlr: 1.250e-03, eta: 0:24:04, time: 0.906, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0039, loss_rpn_bbox: 0.1286, loss_cls: 0.0693, acc: 97.4775, loss_bbox: 0.1163, loss_mask: 0.0868, loss: 0.4049\n",
            "2022-03-18 01:00:21,469 - mmdet - INFO - Epoch [29][100/200]\tlr: 1.250e-04, eta: 0:22:35, time: 0.928, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0031, loss_rpn_bbox: 0.0975, loss_cls: 0.0583, acc: 98.0303, loss_bbox: 0.0986, loss_mask: 0.0833, loss: 0.3408\n",
            "2022-03-18 01:01:51,609 - mmdet - INFO - Epoch [29][200/200]\tlr: 1.250e-04, eta: 0:21:04, time: 0.901, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0032, loss_rpn_bbox: 0.0839, loss_cls: 0.0560, acc: 98.0645, loss_bbox: 0.0904, loss_mask: 0.0833, loss: 0.3168\n",
            "2022-03-18 01:03:24,503 - mmdet - INFO - Epoch [30][100/200]\tlr: 1.250e-04, eta: 0:19:34, time: 0.925, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0029, loss_rpn_bbox: 0.0787, loss_cls: 0.0548, acc: 98.1416, loss_bbox: 0.0894, loss_mask: 0.0841, loss: 0.3099\n",
            "2022-03-18 01:04:55,093 - mmdet - INFO - Epoch [30][200/200]\tlr: 1.250e-04, eta: 0:18:04, time: 0.906, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0027, loss_rpn_bbox: 0.0773, loss_cls: 0.0527, acc: 98.1582, loss_bbox: 0.0869, loss_mask: 0.0821, loss: 0.3016\n",
            "2022-03-18 01:06:27,948 - mmdet - INFO - Epoch [31][100/200]\tlr: 1.250e-04, eta: 0:16:34, time: 0.924, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0026, loss_rpn_bbox: 0.0726, loss_cls: 0.0509, acc: 98.3047, loss_bbox: 0.0852, loss_mask: 0.0838, loss: 0.2952\n",
            "2022-03-18 01:07:58,001 - mmdet - INFO - Epoch [31][200/200]\tlr: 1.250e-04, eta: 0:15:04, time: 0.901, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0023, loss_rpn_bbox: 0.0728, loss_cls: 0.0514, acc: 98.2617, loss_bbox: 0.0852, loss_mask: 0.0818, loss: 0.2935\n",
            "2022-03-18 01:09:30,566 - mmdet - INFO - Epoch [32][100/200]\tlr: 1.250e-04, eta: 0:13:33, time: 0.921, data_time: 0.032, memory: 1971, loss_rpn_cls: 0.0022, loss_rpn_bbox: 0.0675, loss_cls: 0.0508, acc: 98.2510, loss_bbox: 0.0837, loss_mask: 0.0827, loss: 0.2869\n",
            "2022-03-18 01:11:00,794 - mmdet - INFO - Epoch [32][200/200]\tlr: 1.250e-04, eta: 0:12:03, time: 0.902, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0023, loss_rpn_bbox: 0.0692, loss_cls: 0.0492, acc: 98.2881, loss_bbox: 0.0826, loss_mask: 0.0811, loss: 0.2844\n",
            "2022-03-18 01:12:33,659 - mmdet - INFO - Epoch [33][100/200]\tlr: 1.250e-04, eta: 0:10:33, time: 0.924, data_time: 0.032, memory: 1971, loss_rpn_cls: 0.0021, loss_rpn_bbox: 0.0661, loss_cls: 0.0486, acc: 98.3711, loss_bbox: 0.0813, loss_mask: 0.0811, loss: 0.2791\n",
            "2022-03-18 01:14:03,633 - mmdet - INFO - Epoch [33][200/200]\tlr: 1.250e-04, eta: 0:09:02, time: 0.900, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0023, loss_rpn_bbox: 0.0658, loss_cls: 0.0474, acc: 98.3633, loss_bbox: 0.0827, loss_mask: 0.0832, loss: 0.2815\n",
            "2022-03-18 01:15:36,151 - mmdet - INFO - Epoch [34][100/200]\tlr: 1.250e-04, eta: 0:07:32, time: 0.921, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0019, loss_rpn_bbox: 0.0624, loss_cls: 0.0487, acc: 98.3125, loss_bbox: 0.0803, loss_mask: 0.0821, loss: 0.2755\n",
            "2022-03-18 01:17:06,366 - mmdet - INFO - Epoch [34][200/200]\tlr: 1.250e-04, eta: 0:06:01, time: 0.902, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0021, loss_rpn_bbox: 0.0622, loss_cls: 0.0459, acc: 98.3926, loss_bbox: 0.0793, loss_mask: 0.0810, loss: 0.2704\n",
            "2022-03-18 01:18:38,528 - mmdet - INFO - Epoch [35][100/200]\tlr: 1.250e-05, eta: 0:04:31, time: 0.917, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0020, loss_rpn_bbox: 0.0592, loss_cls: 0.0475, acc: 98.3662, loss_bbox: 0.0790, loss_mask: 0.0821, loss: 0.2698\n",
            "2022-03-18 01:20:09,177 - mmdet - INFO - Epoch [35][200/200]\tlr: 1.250e-05, eta: 0:03:00, time: 0.906, data_time: 0.012, memory: 1971, loss_rpn_cls: 0.0021, loss_rpn_bbox: 0.0568, loss_cls: 0.0477, acc: 98.3340, loss_bbox: 0.0774, loss_mask: 0.0801, loss: 0.2642\n",
            "2022-03-18 01:21:41,844 - mmdet - INFO - Epoch [36][100/200]\tlr: 1.250e-05, eta: 0:01:30, time: 0.922, data_time: 0.031, memory: 1971, loss_rpn_cls: 0.0021, loss_rpn_bbox: 0.0579, loss_cls: 0.0459, acc: 98.4443, loss_bbox: 0.0788, loss_mask: 0.0810, loss: 0.2656\n",
            "2022-03-18 01:23:12,050 - mmdet - INFO - Epoch [36][200/200]\tlr: 1.250e-05, eta: 0:00:00, time: 0.902, data_time: 0.013, memory: 1971, loss_rpn_cls: 0.0019, loss_rpn_bbox: 0.0583, loss_cls: 0.0456, acc: 98.4785, loss_bbox: 0.0771, loss_mask: 0.0815, loss: 0.2643\n",
            "2022-03-18 01:23:12,165 - mmdet - INFO - Saving checkpoint at 36 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 5.4 task/s, elapsed: 18s, ETA:     0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-18 01:23:33,508 - mmdet - INFO - Evaluating bbox...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.42s).\n",
            "Accumulating evaluation results...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-18 01:23:34,150 - mmdet - INFO - \n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.347\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.497\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.372\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.231\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.366\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.560\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.560\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.560\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.318\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.544\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.620\n",
            "\n",
            "2022-03-18 01:23:34,151 - mmdet - INFO - Evaluating segm...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE (t=0.21s).\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mmdetection/mmdet/datasets/coco.py:465: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
            "  UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE (t=0.66s).\n",
            "Accumulating evaluation results...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pycocotools/cocoeval.py:378: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n",
            "2022-03-18 01:23:35,082 - mmdet - INFO - \n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.372\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.516\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.419\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.204\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.371\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.583\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.583\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.583\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.400\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.574\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.632\n",
            "\n",
            "2022-03-18 01:23:35,093 - mmdet - INFO - Epoch(val) [36][100]\tbbox_mAP: 0.3470, bbox_mAP_50: 0.4970, bbox_mAP_75: 0.3720, bbox_mAP_s: 0.2310, bbox_mAP_m: 0.3660, bbox_mAP_l: 0.4140, bbox_mAP_copypaste: 0.347 0.497 0.372 0.231 0.366 0.414, segm_mAP: 0.3720, segm_mAP_50: 0.5160, segm_mAP_75: 0.4190, segm_mAP_s: 0.2040, segm_mAP_m: 0.3710, segm_mAP_l: 0.4630, segm_mAP_copypaste: 0.372 0.516 0.419 0.204 0.371 0.463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE (t=0.22s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6pRlioT7p5kd",
        "outputId": "da32fac0-62a5-4a0f-c321-ecd6fcdd8c1c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mmcv\n",
        "from mmcv.runner import load_checkpoint\n",
        "\n",
        "from mmdet.apis import inference_detector, show_result_pyplot\n",
        "from mmdet.models import build_detector\n",
        "\n",
        "# Choose to use a config and initialize the detector\n",
        "config = 'configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco.py'\n",
        "# Setup a checkpoint file to load\n",
        "checkpoint = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
        "\n",
        "# Set the device to be used for evaluation\n",
        "device='cuda:0'\n",
        "\n",
        "# Load the config\n",
        "config = mmcv.Config.fromfile(config)\n",
        "# Set pretrained to be None since we do not need pretrained model here\n",
        "config.model.pretrained = None\n",
        "\n",
        "# Initialize the detector\n",
        "model = build_detector(config.model)\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = load_checkpoint(model, checkpoint, map_location=device)\n",
        "\n",
        "# Set the classes of models for inference\n",
        "model.CLASSES = checkpoint['meta']['CLASSES']\n",
        "\n",
        "# We need to set the model's cfg for inference\n",
        "model.cfg = config\n",
        "\n",
        "# Convert the model to GPU\n",
        "model.to(device)\n",
        "# Convert the model into evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "I42Q8C77qzZP"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}